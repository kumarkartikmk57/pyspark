# Start a simple Spark Session
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('WalmartStockAnalysis').getOrCreate()

# Load the Walmart Stock CSV File, have Spark infer the data types
df = spark.read.csv('walmart_stock.csv', header=True, inferSchema=True)

# What are the column names?
print("Column Names:")
print(df.columns)

# What does the Schema look like?
print("Schema:")
df.printSchema()

# Print out the first 5 columns
print("First 5 columns:")
df.show(5)

# Use describe() to learn about the DataFrame. Provide your inference on the same
print("Describe DataFrame:")
df.describe().show()

# Format the numbers to just show up to two decimal places
from pyspark.sql.functions import format_number
result = df.describe()
result.select(result['summary'],
              format_number(result['Open'].cast('float'), 2).alias('Open'),
              format_number(result['High'].cast('float'), 2).alias('High'),
              format_number(result['Low'].cast('float'), 2).alias('Low'),
              format_number(result['Close'].cast('float'), 2).alias('Close'),
              format_number(result['Volume'].cast('int'), 0).alias('Volume')
             ).show()

# Create a new dataframe with a column called HV Ratio that is the ratio of the High Price versus volume of stock traded for a day
from pyspark.sql.functions import round
df_with_HV_Ratio = df.withColumn('HV Ratio', round(df['High']/df['Volume'], 6))
df_with_HV_Ratio.show(5)

# What day had the Peak High in Price?
print("Day with the Peak High in Price:")
df.orderBy(df['High'].desc()).select(df['Date']).head(1)[0]['Date']

# What is the mean of the Close column?
from pyspark.sql.functions import mean
print("Mean of the Close column:")
df.select(mean('Close')).show()

# What is the max and min of the Volume column?
from pyspark.sql.functions import max, min
print("Max and Min of the Volume column:")
df.select(max('Volume'), min('Volume')).show()

# How many days was the Close lower than 60 dollars?
from pyspark.sql.functions import count
print("Number of days the Close was lower than $60:")
df.filter(df['Close'] < 60).select(count('Date')).show()

# What percentage of the time was the High greater than 80 dollars?
high_greater_than_80 = df.filter(df['High'] > 80).count()
total_days = df.count()
print("Percentage of time High was greater than $80: {}%".format(round(high_greater_than_80/total_days*100, 2)))

# What is the Pearson correlation between High and Volume?
from pyspark.sql.functions import corr
print("Pearson correlation between High and Volume:")
df.select(corr('High', 'Volume')).show()

# What is the max High per year?
from pyspark.sql.functions import year
print("Max High per year:")
df_with_year = df.withColumn('Year', year(df['Date']))
df_with_year.groupBy('Year').max('High').show()

# What is the average Close for each Calendar Month?
from pyspark.sql.functions import month
print("Average Close for each Calendar Month:")
df_with_month = df.withColumn('Month', month(df['Date']))
df_with_month.groupBy('Month').mean('Close').orderBy('Month').show()
